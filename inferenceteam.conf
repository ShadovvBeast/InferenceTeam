[program:inferenceteam]
command=python3 /root/main.py
autostart=true
autorestart=true
stderr_logfile=/var/log/inferenceteam.err.log
stdout_logfile=/var/log/inferenceteam.out.log

[program:llamacpp]
command=/root/llama.cpp/build/bin/server -m /root/llama.cpp/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf --n-gpu-layers 35 --ctx-size 16384 -np 10
autostart=true
autorestart=true
stdout_logfile = /dev/stdout
stdout_logfile_maxbytes = 0
stderr_logfile = /dev/stderr
stderr_logfile_maxbytes = 0
