[program:inferenceteam]
command=python3 /root/main.py
autostart=true
autorestart=true
stderr_logfile=/var/log/inferenceteam.err.log
stdout_logfile=/var/log/inferenceteam.out.log

[program:llamacpp]
command=/root/llama.cpp/build/bin/server -m /root/llama.cpp/models/openhermes-2.5-mistral-7b.Q5_K_M.gguf --n-gpu-layers 35 --ctx-size 4096
autostart=true
autorestart=true
stderr_logfile=/var/log/llamacpp.err.log
stdout_logfile=/var/log/llamacpp.out.log
